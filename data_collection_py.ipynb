{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs7l6Jr4pUiiOQMxqpKzD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/distinct93/Market-basket-Analysis/blob/main/data_collection_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGSx5imm3p7E"
      },
      "outputs": [],
      "source": [
        "\n",
        "  import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "class WebScraper:\n",
        "    def build_url(sort_by, page_number):\n",
        "        sort_by='votes&page='\n",
        "        base_url = \"https://stackoverflow.com/questions?tab=\"\n",
        "        return f'{base_url}{sort_by}{page_number}'\n",
        "\n",
        "    def fetch_page(url):\n",
        "        response = requests.get(url)\n",
        "        return BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    def extract_questions(soup):\n",
        "        classes_needed = ['.s-post-summary--stats-item-number', '.s-post-summary--content-title', '.s-post-summary--content-excerpt', '.post-tag']\n",
        "        key_names = ['votes', 'summary', 'question', 'tags']\n",
        "        question_classes = soup.select('.s-post-summary')\n",
        "        datas = []\n",
        "\n",
        "        for question in question_classes:\n",
        "            question_data = {}\n",
        "            for i, _class in enumerate(classes_needed):\n",
        "                sub_element = question.select_one(_class)\n",
        "                keyname = key_names[i]\n",
        "                question_data[keyname] = sub_element.text if sub_element else None\n",
        "            datas.append(question_data)\n",
        "\n",
        "        return datas\n",
        "\n",
        "    def scrape_stack_overflow(sort_by='votes&page=', start_page=1, end_page=50):\n",
        "        all_data = []\n",
        "        for page_number in range(start_page, end_page + 1):\n",
        "            print(f\"Scraping page {page_number}...\")\n",
        "            url = build_url(sort_by, page_number)\n",
        "            soup = fetch_page(url)\n",
        "            data = extract_questions(soup)\n",
        "            all_data.extend(data)\n",
        "        return pd.DataFrame(all_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    start_page = 1\n",
        "    end_page = 10\n",
        "    questions_df = scrape_stack_overflow(start_page=start_page, end_page=end_page)\n",
        "    print(questions_df)"
      ]
    }
  ]
}